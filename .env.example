# ==============================================================================
# GEETANJALI CONFIGURATION
# ==============================================================================
# Copy this file to .env and update with your values.
#
# This single .env file is used by:
#   - Docker Compose (all services)
#   - Backend (Python/FastAPI via pydantic-settings)
#   - Frontend (React/Vite - VITE_* variables only)
#
# ==============================================================================
# QUICK START - Choose your mode:
# ==============================================================================
#
#   MOCK MODE (UI development only - instant responses, no reasoning):
#     USE_MOCK_LLM=true
#
#   LOCAL-FIRST MODE (recommended for development - no API costs):
#     USE_MOCK_LLM=false
#     LLM_PROVIDER=ollama
#     LLM_FALLBACK_PROVIDER=mock
#     OLLAMA_ENABLED=true
#     # Run: ollama pull qwen2.5:3b
#
#   GEMINI MODE (production - cost-effective, 85% cheaper than Anthropic):
#     USE_MOCK_LLM=false
#     LLM_PROVIDER=gemini
#     LLM_FALLBACK_PROVIDER=anthropic
#     GOOGLE_API_KEY=...
#     ANTHROPIC_API_KEY=sk-ant-...
#
#   ANTHROPIC MODE (production - best quality responses):
#     USE_MOCK_LLM=false
#     LLM_PROVIDER=anthropic
#     LLM_FALLBACK_PROVIDER=gemini
#     ANTHROPIC_API_KEY=sk-ant-...
#     GOOGLE_API_KEY=...
#
# ==============================================================================


# ------------------------------------------------------------------------------
# DATABASE
# ------------------------------------------------------------------------------
# Docker Compose uses POSTGRES_* variables
POSTGRES_DB=geetanjali
POSTGRES_USER=geetanjali
POSTGRES_PASSWORD=geetanjali_dev_pass

# Backend uses DATABASE_URL (localhost for local dev, postgres for Docker)
DATABASE_URL=postgresql://geetanjali:geetanjali_dev_pass@localhost:5432/geetanjali


# ------------------------------------------------------------------------------
# REDIS (Cache)
# ------------------------------------------------------------------------------
REDIS_PASSWORD=redis_dev_pass
# Backend uses REDIS_URL (localhost for local dev, redis for Docker)
REDIS_URL=redis://:redis_dev_pass@localhost:6379/0


# ------------------------------------------------------------------------------
# LLM PROVIDER
# ------------------------------------------------------------------------------
# Primary provider: anthropic | gemini | ollama | mock
# - Development: use "ollama" for local-first (no API costs)
# - Production: use "anthropic" or "gemini" for best quality
#
# Provider routing (automatic):
#   anthropic, gemini → Single-pass (external LLMs, high quality)
#   ollama + MULTIPASS_ENABLED=true → Multipass (5-pass refinement)
#   ollama + MULTIPASS_ENABLED=false → Single-pass
#   mock → Instant test responses
LLM_PROVIDER=ollama

# Intelligent Escalation (v1.34.0+)
# Enable automatic escalation to fallback provider for:
#   - Missing critical response fields (structural failures)
#   - Post-repair confidence below 0.45 threshold
# 4-phase rollout: Phase 1 OFF (0%) → Phase 2 ON (10%) → Phase 3 (25%) → Phase 4 (100%)
FALLBACK_ESCALATION_ENABLED=false

# Fallback provider: anthropic | gemini | ollama | mock
# - Development: use "mock" as fallback when Ollama fails
# - Production: use "gemini" or "ollama" as fallback when primary fails
LLM_FALLBACK_PROVIDER=mock

# Set to true for instant mock responses (UI-only development)
USE_MOCK_LLM=false

# Enable fallback to secondary provider on failure
# Recommended: true for both dev and prod for resilience
LLM_FALLBACK_ENABLED=true

# Include few-shot example in LLM prompts (increases token usage, helps local models)
LLM_USE_FEW_SHOTS=false


# ------------------------------------------------------------------------------
# ANTHROPIC (Claude)
# ------------------------------------------------------------------------------
# Get API key: https://console.anthropic.com/
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-haiku-4-5-20251001
ANTHROPIC_MAX_TOKENS=2048
ANTHROPIC_TIMEOUT=30


# ------------------------------------------------------------------------------
# GEMINI (Google)
# ------------------------------------------------------------------------------
# Get API key: https://aistudio.google.com/apikey
# Cost: ~85% cheaper than Anthropic (Gemini 2.5 Flash)
GOOGLE_API_KEY=
GEMINI_MODEL=gemini-2.5-flash
GEMINI_MAX_TOKENS=4096
GEMINI_TIMEOUT=30


# ------------------------------------------------------------------------------
# OLLAMA (Local LLM)
# ------------------------------------------------------------------------------
# Enable Ollama for local-first development (no API costs)
# Run: ollama pull qwen2.5:3b
OLLAMA_ENABLED=true
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:3b
OLLAMA_TIMEOUT=300
OLLAMA_MAX_TOKENS=1024
OLLAMA_KEEP_ALIVE=5m
# Note: For Docker, use http://host.docker.internal:11434


# ------------------------------------------------------------------------------
# VECTOR DATABASE (ChromaDB)
# ------------------------------------------------------------------------------
CHROMA_HOST=
CHROMA_PORT=8000
CHROMA_PERSIST_DIRECTORY=./chroma_data
CHROMA_COLLECTION_NAME=gita_verses


# ------------------------------------------------------------------------------
# EMBEDDINGS
# ------------------------------------------------------------------------------
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DIMENSION=384

# HuggingFace Token (optional but recommended)
# Without this token, model downloads are rate-limited and you'll see warnings:
#   "You are sending unauthenticated requests to the HF Hub"
# Get a free token at: https://huggingface.co/settings/tokens
# The model is cached after first download, so this mainly affects cold starts.
HF_TOKEN=


# ------------------------------------------------------------------------------
# RAG PIPELINE
# ------------------------------------------------------------------------------
RAG_TOP_K_VERSES=5
RAG_TOP_M_COMMENTARIES=3
RAG_CONFIDENCE_THRESHOLD=0.7
RAG_SCHOLAR_REVIEW_THRESHOLD=0.6

# Follow-up conversation settings
FOLLOW_UP_MAX_TOKENS=1024


# ------------------------------------------------------------------------------
# CONTENT MODERATION
# ------------------------------------------------------------------------------
# Master switch to enable/disable all content filtering
CONTENT_FILTER_ENABLED=true

# Layer 1: Pre-submission blocklist (catches obvious violations before DB write)
CONTENT_FILTER_BLOCKLIST_ENABLED=true

# Layer 1: Profanity/abuse detection (blocks direct offensive language)
CONTENT_FILTER_PROFANITY_ENABLED=true

# Layer 2: LLM refusal detection (detects when Claude refuses inappropriate content)
CONTENT_FILTER_LLM_REFUSAL_DETECTION=true


# ------------------------------------------------------------------------------
# APPLICATION
# ------------------------------------------------------------------------------
APP_ENV=development
DEBUG=true
LOG_LEVEL=INFO

# Base URL for the frontend (used in password reset emails, etc.)
FRONTEND_URL=http://localhost:5173


# ------------------------------------------------------------------------------
# API
# ------------------------------------------------------------------------------
API_V1_PREFIX=/api/v1
CORS_ORIGINS=http://localhost,http://localhost:3000,http://localhost:5173,http://127.0.0.1,http://127.0.0.1:3000,http://127.0.0.1:5173


# ------------------------------------------------------------------------------
# COST DEFENSE: Rate Limiting & Daily Caps (v1.32.0)
# ------------------------------------------------------------------------------
# Conservative defaults designed to prevent bulk automation and cost bleed
# while allowing legitimate single-user scenarios.
# Adjust based on Week 1 monitoring data using docs/COST_DEFENSE.md guide.
#
# Rate limits: How many requests per time window
#   - 3/hour (cases) = 1 consult every 20 min (hostile to scripts)
#   - 5/hour (follow-ups) = natural conversation pace
#
# Daily limits: Hard ceiling per calendar day per IP/session
#   - 20/day = ~2.5/hour average (prevents bulk automation)
#   - Resets at UTC midnight automatically
#
# For more info: see docs/COST_DEFENSE.md

# Hourly rate limits
ANALYZE_RATE_LIMIT=3/hour
FOLLOW_UP_RATE_LIMIT=5/hour

# Daily consumption limit
DAILY_CONSULT_LIMIT=20

# Request token limit (conservative 4 chars/token estimation)
# Prevents oversized submissions from wasting LLM tokens on bloated prompts
REQUEST_TOKEN_LIMIT=2000

# Feature flag (set to false to disable all cost defense checks, e.g., for rollback)
DAILY_CONSULT_LIMIT_ENABLED=true


# ------------------------------------------------------------------------------
# SECURITY (generate secure values for production)
# ------------------------------------------------------------------------------
# Generate: python -c "import secrets; print(secrets.token_hex(32))"

# JWT for user authentication
JWT_SECRET=dev-secret-key-change-in-production
JWT_ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=15
REFRESH_TOKEN_EXPIRE_DAYS=90

# Cookie security (set true in production, requires HTTPS)
COOKIE_SECURE=false

# Admin API key for protected endpoints (/admin/ingest, /admin/sync-featured)
API_KEY=dev-api-key-12345


# ------------------------------------------------------------------------------
# EMAIL (Resend) - for contact form and newsletter
# ------------------------------------------------------------------------------
# Get API key: https://resend.com/
# All three must be set for email features to work
RESEND_API_KEY=
CONTACT_EMAIL_TO=                    # Your email to receive contact messages
CONTACT_EMAIL_FROM=                  # Must use verified domain: Name <email@verified-domain.com>


# ------------------------------------------------------------------------------
# NEWSLETTER
# ------------------------------------------------------------------------------
# Dry-run mode: logs emails without sending (default: true for safety)
# Set to false in production to actually send digest emails
NEWSLETTER_DRY_RUN=true


# ------------------------------------------------------------------------------
# MONITORING (Sentry)
# ------------------------------------------------------------------------------
# Get DSN from: https://sentry.io/
# Backend and frontend can share the same project or use separate ones

# Backend Sentry DSN (Python/FastAPI)
SENTRY_DSN=

# Performance monitoring sample rate (0.0 to 1.0, default 0.1 = 10%)
SENTRY_TRACES_SAMPLE_RATE=0.1


# ------------------------------------------------------------------------------
# OBSERVABILITY (Grafana)
# ------------------------------------------------------------------------------
# Used by docker-compose.observability.yml
# Grafana dashboard: https://grafana.geetanjaliapp.com

# Grafana admin credentials
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=

# Grafana email alerts (uses Resend SMTP - same API key as contact form)
# Set GRAFANA_SMTP_ENABLED=true to enable email alerts
GRAFANA_SMTP_ENABLED=false
GRAFANA_SMTP_FROM=alerts@geetanjaliapp.com
GRAFANA_ALERT_EMAIL_TO=


# ------------------------------------------------------------------------------
# RQ (Background Job Queue)
# ------------------------------------------------------------------------------
RQ_ENABLED=true
RQ_QUEUE_NAME=geetanjali
# 30 minutes for multi-pass pipeline (generous for production)
RQ_JOB_TIMEOUT=1800


# ------------------------------------------------------------------------------
# MULTIPASS CONSULTATION (v1.27.0)
# ------------------------------------------------------------------------------
# 5-pass workflow: Acceptance → Draft → Critique → Refine → Structure
# Enable multi-pass consultation pipeline for Ollama
MULTIPASS_ENABLED=false

# Fall back to single-pass if multi-pass fails completely
MULTIPASS_FALLBACK_TO_SINGLE_PASS=true

# Comparison mode: run both pipelines for quality data collection
# Set to true to collect comparison data for Phase 4 switch decision
MULTIPASS_COMPARISON_MODE=false

# Pass timeouts: Each pass uses OLLAMA_TIMEOUT (or ANTHROPIC_TIMEOUT).
# Total multipass duration = 5 passes × provider timeout.
# RQ_JOB_TIMEOUT must be >= total to avoid premature job termination.

# Pass token limits (reduced for faster local inference)
MULTIPASS_TOKENS_ACCEPTANCE=500
MULTIPASS_TOKENS_DRAFT=1000
MULTIPASS_TOKENS_CRITIQUE=600
MULTIPASS_TOKENS_REFINE=1000
MULTIPASS_TOKENS_STRUCTURE=1200


# ------------------------------------------------------------------------------
# FRONTEND (Vite - only VITE_* variables are exposed to browser)
# ------------------------------------------------------------------------------
# Leave empty - Vite proxy (dev) and nginx (prod) handle /api/ routing
# Only set if you need direct backend access bypassing the proxy
VITE_API_URL=
VITE_API_BASE_URL=
VITE_API_V1_PREFIX=/api/v1

# Frontend Analytics & Monitoring (optional)
VITE_SENTRY_DSN=
VITE_UMAMI_WEBSITE_ID=


# ==============================================================================
# BUDGET DEPLOYMENT (2GB droplet, $12/mo) — ALTERNATIVE CONFIGURATION
# ==============================================================================
# NOTE: This is an alternative for cost optimization. The standard 4GB deployment
# with Ollama (local-first, self-sufficient) remains the recommended configuration.
#
# For 2GB Debian droplets without Ollama (50% cost savings):
#
# Usage:
#   docker compose -f docker-compose.budget.yml up -d
#   docker compose -f docker-compose.budget.yml -f docker-compose.budget.observability.yml up -d
#
# Or via Makefile:
#   make budget-up          # Start services
#   make budget-obs-up      # Start with Prometheus + Grafana
#   make budget-stats       # Check memory usage
#
# Required settings for budget mode:
#   LLM_PROVIDER=gemini                 # Primary (cost-effective)
#   LLM_FALLBACK_PROVIDER=anthropic     # Fallback (quality)
#   OLLAMA_ENABLED=false                # No local LLM (saves ~4GB RAM)
#   GOOGLE_API_KEY=<your-key>           # Required
#   ANTHROPIC_API_KEY=<your-key>        # Required for fallback
#
# Memory budget (~1.6GB for containers, 80% of 2GB):
#   PostgreSQL:  192MB (shared_buffers=48MB, max_connections=30)
#   Redis:       48MB  (maxmemory 40mb)
#   ChromaDB:    640MB (sentence-transformers ~400MB)
#   Backend:     512MB (UVICORN_WORKERS=1 + embedding model ~400MB)
#   Worker:      256MB (separate process)
#   Frontend:    48MB  (nginx static)
#   Total:       ~1.7GB (leaves ~300MB for kernel/Docker)
#   + 1GB swap for spike handling
#
# See docs/deployment.md for full budget deployment guide.


# ==============================================================================
# PRODUCTION CHECKLIST
# ==============================================================================
# Before deploying to production, ensure:
#
# 1. [ ] Generate secure secrets:
#        python -c "import secrets; print(secrets.token_hex(32))"
#        - Set JWT_SECRET to generated value
#        - Set API_KEY to generated value
#        - Set POSTGRES_PASSWORD to secure password
#        - Set REDIS_PASSWORD to secure password
#
# 2. [ ] Configure LLM:
#        - Set USE_MOCK_LLM=false
#        - Set LLM_PROVIDER=anthropic or gemini
#        - Set LLM_FALLBACK_PROVIDER=gemini or anthropic (cross-fallback)
#        - Set ANTHROPIC_API_KEY for Claude
#        - Set GOOGLE_API_KEY for Gemini
#        - Optional: Set OLLAMA_ENABLED=true for local fallback
#
# 3. [ ] Set production values:
#        - APP_ENV=production
#        - DEBUG=false
#        - COOKIE_SECURE=true (requires HTTPS)
#        - LOG_LEVEL=WARNING
#
# 4. [ ] Configure CORS_ORIGINS with your domain(s)
#
# 5. [ ] Set FRONTEND_URL to your production domain
#
# 6. [ ] Configure email (optional but recommended):
#        - Set RESEND_API_KEY from https://resend.com/
#        - Set CONTACT_EMAIL_TO to your support email
#        - Set CONTACT_EMAIL_FROM with your domain
#
# 7. [ ] Configure newsletter (if using daily digest):
#        - Set NEWSLETTER_DRY_RUN=false to enable sending
#        - Requires RESEND_API_KEY to be configured
#
# 8. [ ] Configure error monitoring (recommended):
#        - Set SENTRY_DSN for backend error tracking
#        - Set VITE_SENTRY_DSN for frontend error tracking
#
# 9. [ ] Configure observability (optional but recommended):
#        - Set GRAFANA_ADMIN_PASSWORD to a secure password
#        - Deploy with: make obs-up
#        - Access Grafana at https://grafana.geetanjaliapp.com
